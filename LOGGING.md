# è¯¦ç»†æ—¥å¿—åŠŸèƒ½ä½¿ç”¨æŒ‡å—

## æ¦‚è¿°

nano-vllm ç°å·²æ·»åŠ å…¨é¢çš„æ—¥å¿—ç³»ç»Ÿï¼Œå¯ä»¥è¿½è¸ªæ•´ä¸ªæ¨ç†æµç¨‹çš„è¯¦ç»†ä¿¡æ¯ï¼ŒåŒ…æ‹¬ï¼š

- **è¾“å…¥è¾“å‡ºå½¢çŠ¶** - å„ä¸ªé˜¶æ®µçš„å¼ é‡ç»´åº¦
- **å†…å­˜å ç”¨** - GPUå†…å­˜åˆ†é…å’Œä½¿ç”¨æƒ…å†µ
- **é€šä¿¡ç»†èŠ‚** - å¼ é‡å¹¶è¡Œçš„é€šä¿¡æ“ä½œï¼ˆå¦‚æœå¯ç”¨ï¼‰
- **è°ƒåº¦å†³ç­–** - æ‰¹æ¬¡ç»„æˆå’Œåºåˆ—è°ƒåº¦
- **å—ç®¡ç†** - KV cacheå—çš„åˆ†é…å’Œç¼“å­˜å‘½ä¸­
- **æ‰§è¡Œæ—¶é—´** - å…³é”®æ“ä½œçš„è€—æ—¶

### ğŸ¯ æ™ºèƒ½æ—¥å¿—é¢‘ç‡æ§åˆ¶

ä¸ºäº†é¿å…DECODEé˜¶æ®µäº§ç”Ÿè¿‡å¤šé‡å¤æ—¥å¿—ï¼Œç³»ç»Ÿé‡‡ç”¨äº†æ™ºèƒ½é¢‘ç‡æ§åˆ¶ï¼š

**INFOçº§åˆ«ï¼ˆé»˜è®¤ï¼‰**ï¼š
- PREFILLæ‰¹æ¬¡ï¼šæ¯æ¬¡éƒ½è®°å½•ï¼ˆè¾ƒå°‘å‘ç”Ÿï¼‰
- DECODEæ‰¹æ¬¡ï¼šä»…åœ¨å…³é”®æ—¶åˆ»è®°å½•
  - âœ… ç¬¬ä¸€ä¸ªdecode step
  - âœ… æ¯10ä¸ªstep
  - âœ… æ‰¹æ¬¡å¤§å°æ”¹å˜æ—¶
  - âœ… å‘ç”Ÿåºåˆ—æŠ¢å æ—¶
- ModelRunnerçš„è¯¦ç»†ä¿¡æ¯ï¼ˆå¼ é‡å½¢çŠ¶ç­‰ï¼‰è‡ªåŠ¨é™çº§åˆ°DEBUG

**DEBUGçº§åˆ«**ï¼š
- è®°å½•æ‰€æœ‰stepçš„è¯¦ç»†ä¿¡æ¯
- åŒ…æ‹¬æ‰€æœ‰å¼ é‡å½¢çŠ¶å’Œä¸­é—´ç»“æœ

è¿™æ ·å¯ä»¥ä¿æŒæ—¥å¿—ç®€æ´æ˜“è¯»ï¼ŒåŒæ—¶åœ¨éœ€è¦æ—¶ä»èƒ½çœ‹åˆ°å®Œæ•´ç»†èŠ‚ã€‚

## æ—¥å¿—çº§åˆ«æ§åˆ¶

### æ–¹å¼1ï¼šç¯å¢ƒå˜é‡ï¼ˆæ¨èï¼‰

```bash
# Windows PowerShell
$env:NANOVLLM_LOG_LEVEL="DEBUG"
python example.py

# Linux/Mac
export NANOVLLM_LOG_LEVEL=DEBUG
python example.py
```

### æ–¹å¼2ï¼šåœ¨ä»£ç ä¸­è®¾ç½®

```python
import os
os.environ["NANOVLLM_LOG_LEVEL"] = "DEBUG"

# ç„¶åå¯¼å…¥ nanovllm
from nanovllm import LLM, SamplingParams
```

### å¯ç”¨çš„æ—¥å¿—çº§åˆ«

- **`DEBUG`** - æœ€è¯¦ç»†ï¼Œæ˜¾ç¤ºæ‰€æœ‰æ—¥å¿—ï¼ˆåŒ…æ‹¬å¼ é‡å½¢çŠ¶ã€æ¯ä¸ªstepçš„ç»†èŠ‚ï¼‰
- **`INFO`** - æ ‡å‡†è¯¦ç»†ç¨‹åº¦ï¼Œæ˜¾ç¤ºä¸»è¦æµç¨‹å’Œç»Ÿè®¡ä¿¡æ¯
- **`WARNING`** - ä»…æ˜¾ç¤ºè­¦å‘Šå’Œé”™è¯¯
- **`ERROR`** - ä»…æ˜¾ç¤ºé”™è¯¯

## æ—¥å¿—å†…å®¹è¯´æ˜

### 1. åˆå§‹åŒ–é˜¶æ®µ

```
[INFO] ================================================================================
[INFO] Initializing LLMEngine
[INFO] Model: ~/huggingface/Qwen3-0.6B/
[INFO] Configuration: tensor_parallel_size=1, max_model_len=2048, ...
[INFO] Initializing ModelRunner (rank 0)
[INFO] Loading model: ~/huggingface/Qwen3-0.6B/
[INFO] Model config: num_layers=28, hidden_size=896, ...
[INFO] After model loading - GPU Memory: Allocated=X.XXG B, Reserved=X.XXGB
[INFO] Allocating KV cache
[INFO] KV cache configuration: num_kv_heads=2, head_dim=128, block_size=16
[INFO] Allocating 512 KV cache blocks, total_size=X.XXGB
[INFO] After KV cache allocation - GPU Memory: Allocated=X.XXGB
```

**å…³é”®ä¿¡æ¯**ï¼š
- æ¨¡å‹é…ç½®å‚æ•°
- åŠ è½½åçš„GPUå†…å­˜å ç”¨
- KV cacheçš„é…ç½®å’Œå¤§å°
- åˆ†é…åçš„GPUå†…å­˜å ç”¨

### 2. ç”Ÿæˆé˜¶æ®µ

```
[INFO] --------------------------------------------------------------------------------
[INFO] Starting generation for 2 prompts
[INFO] Total prompt tokens: 156
```

**å…³é”®ä¿¡æ¯**ï¼š
- æ‰¹æ¬¡ä¸­çš„promptæ•°é‡
- æ€»çš„è¾“å…¥tokenæ•°

### 3. Prefillé˜¶æ®µï¼ˆé¦–æ¬¡å¤„ç†æç¤ºè¯ï¼‰

```
[INFO] Scheduled PREFILL batch: size=2, batched_tokens=156, 
      seqs=[seq_0(len=78,cached=0), seq_1(len=78,cached=0)]
[DEBUG] [Rank 0] Preparing PREFILL batch: 2 sequences
[DEBUG] [Rank 0] PREFILL tensors: input_ids=[156], positions=[156], slot_mapping=[156]
[DEBUG] [Rank 0] PREFILL: max_seqlen_q=78, max_seqlen_k=78, total_tokens=156
[DEBUG] [START] [Rank 0] Model forward PREFILL
[DEBUG] [END] [Rank 0] Model forward PREFILL - Elapsed: XX.XXms
[DEBUG] [Rank 0] Output: logits=[156, 151936]
```

**å…³é”®ä¿¡æ¯**ï¼š
- Prefillæ‰¹æ¬¡å¤§å°å’Œåºåˆ—ä¿¡æ¯
- è¾“å…¥å¼ é‡çš„å½¢çŠ¶
- åºåˆ—çš„æœ€å¤§é•¿åº¦
- æ¨¡å‹å‰å‘ä¼ æ’­çš„è€—æ—¶
- è¾“å‡ºlogitsçš„å½¢çŠ¶

### 4. Decodeé˜¶æ®µï¼ˆé€tokenç”Ÿæˆï¼‰

**INFOçº§åˆ«ï¼ˆç®€æ´æ¨¡å¼ï¼‰**ï¼š
```
[INFO] Scheduled DECODE batch (step 1): size=2, preempted=0, seqs=[seq_0(len=79), seq_1(len=79)]
... (æ­¥éª¤2-9ï¼šä»…DEBUGçº§åˆ«è®°å½•)
[INFO] Scheduled DECODE batch (step 10): size=2, preempted=0, seqs=[seq_0(len=88), seq_1(len=88)]
... (æ­¥éª¤11-19ï¼šä»…DEBUGçº§åˆ«è®°å½•)
[INFO] Scheduled DECODE batch (step 20): size=2, preempted=0, seqs=[seq_0(len=98), seq_1(len=98)]
```

**DEBUGçº§åˆ«ï¼ˆè¯¦ç»†æ¨¡å¼ï¼‰**ï¼š
```
[INFO] Scheduled DECODE batch (step 1): size=2, preempted=0, seqs=[seq_0(len=79), seq_1(len=79)]
[DEBUG] [Rank 0] Preparing DECODE batch: 2 sequences
[DEBUG] [Rank 0] DECODE tensors: input_ids=[2], positions=[2], context_lens=[2], block_tables=[2, 5]
[DEBUG] [START] [Rank 0] Model forward DECODE
[DEBUG] Decode attention: k_cache=[2, 28, 512, 16, 2, 128], v_cache=[2, 28, 512, 16, 2, 128]
[DEBUG] [END] [Rank 0] Model forward DECODE - Elapsed: XX.XXms
[DEBUG] [Rank 0] Output: logits=[2, 151936]
[DEBUG] Scheduled DECODE batch (step 2): size=2, preempted=0, seqs=[seq_0(len=80), seq_1(len=80)]
... (æ¯ä¸ªstepéƒ½æœ‰è¯¦ç»†æ—¥å¿—)
```

**å…³é”®ä¿¡æ¯**ï¼š
- Decodeæ‰¹æ¬¡ä¿¡æ¯ï¼ˆå‘¨æœŸæ€§è®°å½•ï¼‰
- è¾“å…¥å¼ é‡å½¢çŠ¶ï¼ˆDEBUGçº§åˆ«ï¼‰
- Contexté•¿åº¦ï¼ˆDEBUGçº§åˆ«ï¼‰
- KV cacheçš„å®Œæ•´å½¢çŠ¶ï¼ˆDEBUGçº§åˆ«ï¼‰
- Decodeçš„è€—æ—¶ï¼ˆDEBUGçº§åˆ«ï¼‰

### 5. å—ç®¡ç†

```
[DEBUG] Allocating blocks for seq_id=0, num_blocks=5
[DEBUG] Allocated blocks for seq_id=0: new=5, cache_hits=0, 
        free_blocks=507, used_blocks=5
[DEBUG] Deallocated blocks for seq_id=0: freed=5/5, free_blocks=512
```

**å…³é”®ä¿¡æ¯**ï¼š
- ä¸ºæ¯ä¸ªåºåˆ—åˆ†é…çš„å—æ•°
- æ–°åˆ†é…çš„å—å’Œç¼“å­˜å‘½ä¸­çš„å—
- ç©ºé—²å’Œå·²ä½¿ç”¨çš„å—æ•°é‡

### 6. Attentionå±‚

```
[DEBUG] Attention forward: is_prefill=True, q=[156, 16, 64], k=[156, 2, 64], v=[156, 2, 64]
[DEBUG] Storing KV cache: N=156, num_heads=2, head_dim=64, num_slots=156
[DEBUG] Attention output: [156, 16, 64]
```

**å…³é”®ä¿¡æ¯**ï¼š
- æ˜¯Prefillè¿˜æ˜¯Decodeæ¨¡å¼
- Queryã€Keyã€Valueçš„å½¢çŠ¶
- å­˜å‚¨åˆ°KV cacheçš„æ•°é‡
- Attentionè¾“å‡ºçš„å½¢çŠ¶

### 7. åºåˆ—å®Œæˆ

```
[DEBUG] Sequence finished: seq_id=0, output_len=256, reason=max_tokens
[DEBUG] Postprocess: 1/2 sequences finished
```

**å…³é”®ä¿¡æ¯**ï¼š
- å®Œæˆçš„åºåˆ—ID
- ç”Ÿæˆçš„tokenæ•°é‡
- å®ŒæˆåŸå› ï¼ˆEOSæˆ–è¾¾åˆ°max_tokensï¼‰

### 8. æœ€ç»ˆç»Ÿè®¡

```
[INFO] Generation complete: 42 steps, total_output_tokens=512, 
       prefill_time=0.15s, decode_time=2.43s
[INFO] After generation - GPU Memory: Allocated=X.XXGB
[INFO] --------------------------------------------------------------------------------
```

**å…³é”®ä¿¡æ¯**ï¼š
- æ€»æ­¥æ•°
- ç”Ÿæˆçš„æ€»tokenæ•°
- Prefillå’ŒDecodeçš„æ€»è€—æ—¶
- æœ€ç»ˆGPUå†…å­˜å ç”¨

## ç¤ºä¾‹è¾“å‡º

è¿è¡Œ `example.py` çš„å®Œæ•´æ—¥å¿—ç¤ºä¾‹ï¼š

```bash
python example.py
```

ä¼šçœ‹åˆ°ç±»ä¼¼ä»¥ä¸‹çš„è¾“å‡ºï¼ˆç®€åŒ–ç‰ˆï¼‰ï¼š

```
[2026-01-11 17:40:00] [nanovllm.engine.llm_engine] [INFO] ================================================================================
[2026-01-11 17:40:00] [nanovllm.engine.llm_engine] [INFO] Initializing LLMEngine
[2026-01-11 17:40:00] [nanovllm.engine.llm_engine] [INFO] Model: ~/huggingface/Qwen3-0.6B/
...
[2026-01-11 17:40:05] [nanovllm.engine.model_runner] [INFO] After KV cache allocation - GPU Memory [Device 0]: Allocated=2.15GB
...
[2026-01-11 17:40:05] [nanovllm.engine.llm_engine] [INFO] Starting generation for 2 prompts
[2026-01-11 17:40:05] [nanovllm.engine.scheduler] [INFO] Scheduled PREFILL batch: size=2, batched_tokens=156
...
[2026-01-11 17:40:08] [nanovllm.engine.llm_engine] [INFO] Generation complete: 42 steps, total_output_tokens=512
```

## æ€§èƒ½è°ƒä¼˜å»ºè®®

1. **ç”Ÿäº§ç¯å¢ƒ**ï¼šä½¿ç”¨ `WARNING` çº§åˆ«ä»¥å‡å°‘æ—¥å¿—å¼€é”€
2. **è°ƒè¯•/åˆ†æ**ï¼šä½¿ç”¨ `DEBUG` çº§åˆ«æŸ¥çœ‹æ‰€æœ‰ç»†èŠ‚
3. **ä¸€èˆ¬ä½¿ç”¨**ï¼šä½¿ç”¨ `INFO` çº§åˆ«è·å¾—ä¸»è¦æµç¨‹ä¿¡æ¯

## è‡ªå®šä¹‰æ—¥å¿—

å¦‚æœéœ€è¦è‡ªå®šä¹‰æ—¥å¿—æ ¼å¼ï¼Œå¯ä»¥åœ¨å¯¼å…¥ nanovllm ä¹‹å‰è®¾ç½®ï¼š

```python
import logging

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s | %(levelname)s | %(name)s | %(message)s',
    datefmt='%H:%M:%S'
)

from nanovllm import LLM, SamplingParams
```

## æ•…éšœæ’æŸ¥

ä½¿ç”¨è¯¦ç»†æ—¥å¿—å¯ä»¥å¸®åŠ©è¯Šæ–­é—®é¢˜ï¼š

- **å†…å­˜ä¸è¶³**ï¼šæŸ¥çœ‹ "GPU Memory" æ—¥å¿—äº†è§£å†…å­˜ä½¿ç”¨å³°å€¼
- **æ€§èƒ½é—®é¢˜**ï¼šæŸ¥çœ‹ "Elapsed" æ—¥å¿—æ‰¾å‡ºç“¶é¢ˆ
- **è°ƒåº¦é—®é¢˜**ï¼šæŸ¥çœ‹ Scheduler çš„æ—¥å¿—äº†è§£æ‰¹æ¬¡ç»„æˆ
- **ç¼“å­˜æ•ˆç‡**ï¼šæŸ¥çœ‹ BlockManager çš„ cache_hits äº†è§£ç¼“å­˜å‘½ä¸­ç‡
